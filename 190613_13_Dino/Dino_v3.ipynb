{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "\n",
    "** This problem comes from Andrew Ng's coursera course. Instead of using their Keras-based solution, we will try to solve this problem based on Tensorflow.**\n",
    "\n",
    "Imagine that leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. \n",
    "\n",
    "\n",
    "By completing this assignment you will learn:\n",
    "\n",
    "- How to store text data for processing using an RNN \n",
    "- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit\n",
    "- How to build a character-level text generation recurrent neural network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from utils import *\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 - Problem Statement\n",
    "\n",
    "### 1.1 - Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19912 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\" (or newline character), which in this assignment plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. \n",
    "In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, `char_to_ix` and `ix_to_char` are the python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building the language model \n",
    "\n",
    "It is time to build the character-level language model for text generation. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1 - Build training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. The following function build_training_data() generates X_train, Y_train, and seqlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data( filename, name_length=30 ):\n",
    "\n",
    "    with open(filename) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]    \n",
    "    \n",
    "    data_ix = []   \n",
    "    for name in examples:\n",
    "        data_ix.append([char_to_ix[ch] for ch in name])\n",
    "\n",
    "    seqlen = []\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for name_ix in data_ix:  \n",
    "        seqlen.append( len(name_ix) )\n",
    "        x = name_ix.copy()\n",
    "        x.extend([np.nan]*(name_length-len(name_ix)))\n",
    "        X_train.append( x )\n",
    "        y = name_ix[1:].copy()+[char_to_ix[\"\\n\"]]\n",
    "        y.extend([np.nan]*(name_length-len(name_ix)))\n",
    "        Y_train.append( y )\n",
    "    \n",
    "    X_train, Y_train, seqlen = np.array(X_train), np.array(Y_train), np.array(seqlen)\n",
    "    \n",
    "    return X_train, Y_train, seqlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Build the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders X_tr, Y_tr, Inits_tr, and Seq_len_tr for training data. In addition, create placeholders X_te, Y_te, Inits_te, and Seq_len_te. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_a, n_layers):\n",
    "    \n",
    "    # create placeholder for switch\n",
    "    is_training = tf.placeholder_with_default( tf.constant(True), [], name='is_training' )\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Use tf.placeholder().\n",
    "    # create placeholder for X_tr. dtype:tf.int32, shape:[None,None] (actually [M, n_steps]), name='X_tr'\n",
    "    X_tr = tf.placeholder( tf.int32, [None, None], name='X_tr' ) \n",
    "    # create placeholder for Y_tr. dtype:tf.int32, shape:[None,None] (actually [M, n_steps]), name='Y_tr'\n",
    "    Y_tr = tf.placeholder( tf.int32, [None, None], name='Y_tr') \n",
    "    # create placeholder for Inits_tr. dtype:tf.float32, shape:[None,n_a*n_layers] (actually [M,n_a*n_layers]), name='Inits_tr'\n",
    "    Inits_tr = tf.placeholder( tf.float32, [None, n_a*n_layers], name='Inits_tr')  \n",
    "    # create placeholder for Seq_len_tr. dtype:tf.int32, shape:[None] (actually for [M]), name='Seq_len_tr'\n",
    "    Seq_len_tr = tf.placeholder( tf.int32, [None], name='Seq_len_tr' ) \n",
    "    ### ENE CODE HERE ###\n",
    "    \n",
    "    # create placeholders for testing\n",
    "    # X_te:(1, 1), Inits_te:(1, 200), Seq_len_te:(1,)\n",
    "    X_te = tf.placeholder_with_default( tf.constant([[1]], dtype=tf.int32), [1, 1], name='X_te' ) \n",
    "    Y_te = tf.placeholder_with_default( tf.constant([[1]], dtype=tf.int32), [None, None], name='Y_te')\n",
    "    Inits_te = tf.placeholder_with_default( tf.zeros([1, n_a*n_layers], dtype=tf.float32), [1, n_a*n_layers], name='Inits_te')  \n",
    "    Seq_len_te = tf.placeholder_with_default( tf.constant([1], dtype=tf.int32), [1], name='Seq_len_te' ) \n",
    "    \n",
    "    return (is_training, X_tr, Y_tr, Inits_tr, Seq_len_tr, X_te, Y_te, Inits_te, Seq_len_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr = Tensor(\"X_tr:0\", shape=(?, ?), dtype=int32)\n",
      "Y_tr = Tensor(\"X_tr:0\", shape=(?, ?), dtype=int32)\n",
      "Inits_tr = Tensor(\"Inits_tr:0\", shape=(?, 20), dtype=float32)\n",
      "Seq_len_tr = Tensor(\"Seq_len_tr:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "(is_training, X_tr, Y_tr, Inits_tr, Seq_len_tr, X_te, Y_te, Inits_te, Seq_len_te) = create_placeholders(10, 2)\n",
    "print (\"X_tr = \" + str(X_tr))\n",
    "print (\"Y_tr = \" + str(X_tr))\n",
    "print (\"Inits_tr = \" + str(Inits_tr))\n",
    "print (\"Seq_len_tr = \" + str(Seq_len_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X_tr = Tensor(\"X_tr_1:0\", shape=(?, ?), dtype=int32)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y_tr = Tensor(\"X_tr_1:0\", shape=(?, ?), dtype=int32)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Inits_tr = Tensor(\"Inits_tr_1:0\", shape=(?, 20), dtype=float32)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Seq_len_tr = Tensor(\"Seq_len_tr_1:0\", shape=(?,), dtype=int32)\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the forward_propagation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(Xo, n_layers, n_a, inits, seq_len, n_y):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Create the list of GRUCells using tf.nn.rnn_cell.GRUCell. \n",
    "    # The number of hidden nodes is n_a and the number of layers is n_layers.\n",
    "    basic_cell = [tf.nn.rnn_cell.GRUCell(n_a) for _ in range(n_layers)]\n",
    "    # Create multi-RNN cell using tf.nn.rnn_cell.MultiRNNCell. Set state_is_tuple to False.(should be updated)\n",
    "    basic_cell = tf.nn.rnn_cell.MultiRNNCell( basic_cell, state_is_tuple=False ) \n",
    "    # Build dynamic RNNs using tf.nn.dynamic_rnn(). Be careful with the arguments, initial_state, dtype(tf.float32), seq_len.\n",
    "    # It returns outpus and states where outputs.shape = [batch_size, n_steps, num_units], states.shape = [batch_size, n_a]\n",
    "    outputs, states = tf.nn.dynamic_rnn( basic_cell, Xo, initial_state=inits, dtype = tf.float32, sequence_length=seq_len )\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Yfloat shape : [ batch_size x n_steps, n_a ]\n",
    "    Yflat = tf.reshape(outputs, [-1, n_a])    \n",
    "    # Ylogits shape : [ batch_size x n_steps, vocab_size ] \n",
    "    Ylogits = tf.contrib.layers.fully_connected( Yflat, n_y, activation_fn = None )\n",
    "    \n",
    "    return Ylogits, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_out.shape:  (4, 3)\n",
      "states_out.shape: (1, 5)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(1)\n",
    "Xo = tf.one_hot([[1, 2, 0, 0]], 3)\n",
    "Z3 = forward_propagation( Xo, 1, 5, tf.constant([[0.0,0.0,0.0,0.0,0.0]],dtype=tf.float32), tf.constant([2],dtype=tf.int32), 3 )\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    logit_out, states_out = sess.run(Z3)\n",
    "print(\"logit_out.shape: \", logit_out.shape)\n",
    "print(\"states_out.shape:\", states_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    logit_out.shape:  (4, 3)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    states_out.shape: (1, 5)\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the loss. Ylogits has the shape [batch_size*n_steps, n_y] and thus Yo also should be reshaped to have the same shape.\n",
    "Use tf.reshape(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Ylogits, Yo, n_y):\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Yflat_ shape : [ batch_size x n_steps, n_y ]\n",
    "    # Use tf.reshape() with [-1, n_y]\n",
    "    Yflat_ = tf.reshape(Yo, [-1, n_y])     \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Compute loss. At the following first line, loss has the shape [ batch_size x n_steps ]\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  \n",
    "    loss = tf.reduce_mean(loss)    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-0e1495331f63>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "loss_out:  0.34115392\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(1)\n",
    "logits = tf.constant([0.8,-0.1], dtype=tf.float32)\n",
    "Y = tf.constant([1,0], dtype=tf.float32)\n",
    "loss = compute_loss( logits, Y, 2 )\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_out = sess.run(loss)\n",
    "print(\"loss_out: \", loss_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    loss_out:  0.34115392\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model by adding the functions you built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model( ix_to_char, char_to_ix, num_epochs = 35000, n_a = 100, \n",
    "          dino_names = 30, vocab_size = 27, batch_size=100, learning_rate=0.001, max_len_name=30, n_layers=2):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_epochs -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    batch_size\n",
    "    learning_rate\n",
    "    max_len_name\n",
    "    n_layers -- number of layers of RNNs\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size (=27)\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Build training data with padding\n",
    "    # X_train' shape : [m, n_steps]\n",
    "    # Y_train' shape : [m, n_steps]\n",
    "    X_train, Y_train, seqlen_train = build_training_data( \"dinos.txt\" )\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    # Create placeholder\n",
    "    (is_training, X_tr, Y_tr, Inits_tr, Seq_len_tr, X_te, Y_te, Inits_te, Seq_len_te) = create_placeholders( n_a, n_layers )\n",
    "\n",
    "    # Create training dataset and its iterator\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_tr, Y_tr, Inits_tr, Seq_len_tr))\n",
    "    train_dataset = train_dataset.repeat().batch(batch_size)\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    train_iter_init_op = train_iterator.make_initializer(train_dataset, name=\"train_iter_init_op\")\n",
    "    \n",
    "    # Create test dataset and its iterator\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_te, Y_te, Inits_te, Seq_len_te))\n",
    "    test_dataset = test_dataset.repeat().batch(1)\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "    test_iter_init_op = test_iterator.make_initializer(test_dataset, name=\"test_iter_init_op\")\n",
    "    \n",
    "    # Execute conditionally\n",
    "    (x, y, inits, seq_len) = tf.cond(tf.equal(is_training, tf.constant(True, dtype=tf.bool)), \n",
    "                                     lambda: train_iterator.get_next(), \n",
    "                                     lambda: test_iterator.get_next())\n",
    "    \n",
    "    # Change x and y into one-hot codes\n",
    "    Xo = tf.one_hot( x, n_x ) #[m, n_steps, n_x]\n",
    "    Yo = tf.one_hot( y, n_y ) #[m, n_steps, n_y]        \n",
    "    \n",
    "    # Build the model for forward propagation\n",
    "    # Ylogits's shape : [ batch_size x n_steps, n_y=vocab_size ] \n",
    "    Ylogits, states = forward_propagation(Xo, n_layers, n_a, inits, seq_len, n_y)\n",
    "    \n",
    "    # compute lost\n",
    "    loss = compute_loss(Ylogits, Yo, n_y)\n",
    "    \n",
    "    # optimize the model\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # compute the number of minibatches\n",
    "        num_minibatches = np.ceil(m / batch_size).astype(int)         \n",
    "\n",
    "        # Run the initialization\n",
    "        states_zero = np.zeros((X_train.shape[0], n_a*n_layers))\n",
    "        print(\"X_train:{}, Y_train:{}, Inits:{}, Seq_len:{}\"\n",
    "              .format(X_train.shape, Y_train.shape, states_zero.shape, seqlen_train.shape))\n",
    "\n",
    "        sess.run([tf.global_variables_initializer(), train_iter_init_op, test_iter_init_op], \n",
    "                 feed_dict={X_tr:X_train, Y_tr:Y_train, Inits_tr:states_zero, Seq_len_tr:seqlen_train })\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            train_dataset.shuffle(100000)\n",
    "            minibatch_loss = 0.            \n",
    "            for i in range(num_minibatches):\n",
    "                # run session. use sess.run()\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_loss = sess.run( [train_op, loss] )\n",
    "\n",
    "                minibatch_loss += temp_loss\n",
    "            minibatch_loss /= num_minibatches\n",
    "            \n",
    "            # Print the cost every epoch\n",
    "            if epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_loss))\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                ry = np.array([[np.random.randint(0, 27)]])\n",
    "                rh = np.zeros([1, n_a*n_layers])\n",
    "                for name in range(dino_names):\n",
    "                    sess.run(test_iter_init_op, feed_dict={X_te: ry, Inits_te: rh, Seq_len_te:[1], is_training:False} )\n",
    "                    ry_logits, rh, x_out, is_training_out, X_te_out = sess.run([Ylogits, states, x, is_training, X_te],\n",
    "                                                feed_dict={X_te: ry, Inits_te: rh, Seq_len_te:[1], is_training:False})\n",
    "                    ry = np.argmax(ry_logits, axis=1)\n",
    "                    if ix_to_char[ry[0]]=='\\n':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(ix_to_char[ry[0]], end = ' ')\n",
    "                        ry = np.reshape(ry, (1,1))\n",
    "                print('\\n')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. 100 or 200 epoch may be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(1539, 30), Y_train:(1539, 30), Inits:(1539, 200), Seq_len:(1539,)\n",
      "Cost after epoch 0: 1.254533\n",
      "s u u \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model( ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like `maconucon`, `marloralus` and `macingsersaurus`. Your model hopefully also learned that dinosaur names tend to end in `saurus`, `don`, `aura`, `tor`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
